### Top 4 Final Projects for AI Infra & Cloud Engineer Roles (100% Free, Local-Only Setup: Target Oct 2026)

With ~10 months ahead (from Nov 26, 2025), these rewritten projects eliminate all cloud costs by using **local/free alternatives** like Minikube for Kubernetes, LocalStack for AWS emulation, Google Colab for GPU access, and open-source tools on your laptop. No credit cards, no tiers—everything runs on your machine (assume 8GB+ RAM; use WSL on Windows or native Linux/Mac). This keeps demos production-like for interviews (e.g., via GitHub + screenshots/videos). Dedicate 1-2 months per project (3-4 hrs/day), tracking in Notion.

Your Go/Rust skills shine: Use Go for scripts, Rust for efficient modules. Prototype everything locally first—record demos with OBS Studio (free).

#### Overall Skills to Learn (Free-Focused Roadmap)
- **Months 1-2 (Foundation)**: Docker basics, Minikube/K3s for local K8s, Python ML (PyTorch via pip). Resource: freeCodeCamp's "Docker Tutorial" (YouTube).
- **Months 3-5 (Core)**: Local MLOps (Kubeflow on Minikube), CI/CD (GitHub Actions free), Monitoring (Prometheus local). Cert: Free Kubernetes.io interactive tutorials.
- **Months 6-8 (Advanced)**: Distributed sim (Ray local), Serverless emulation (LocalStack), GPU via Colab. Book: "Kubernetes in Action" by Marko Lukša (free PDF via libgen or borrow).
- **Months 9-10 (Polish)**: Local system designs, security basics. Resource: "Designing Data-Intensive Applications" by Martin Kleppmann (free via archive.org).
- **Daily Practice**: 1 hr DSA (LeetCode), 1 hr ByteByteGo (free newsletter), weekly GitHub pushes.

| Project # | Project Name & Description (Fully Local/Free)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Key Skills Gained                                                                                        | Estimated Time & Tech Stack                                                            | Resources/Books                                                                                                                                                                                                                                                        |
| --------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
|           | **Local MLOps Pipeline for Image Classification**<br>Build a CI/CD pipeline to train/deploy a PyTorch-based image classifier (e.g., CIFAR-10) on a local Minikube Kubernetes cluster. Automate data prep (ETL with Pandas), model training (Hugging Face), versioning (MLflow local server), and serving (via FastAPI endpoint). Use GitHub Actions for CI/CD; simulate auto-scaling with Minikube's resource limits. Twist: Add Go script for local pipeline orchestration. Demo: Run inferences locally and record video of scaling.                               | Containerization (Docker/Minikube), Local MLOps, CI/CD (GitHub Actions), API serving.                    | 4-6 weeks<br>Python/Go, Docker, Kubeflow (on Minikube), MLflow (local).                | - Tutorial: Minikube docs (minikube.sigs.k8s.io/docs/tutorials/multi_node).<br>- Book: "Machine Learning Engineering in Action" by Ben Wilson (Ch. 4-6; free via libgen).<br>- Video: freeCodeCamp's "MLOps with Minikube" (YouTube, 2 hrs).                           |
| **2**     | **Emulated Serverless RAG Chatbot Infra**<br>Deploy a Retrieval-Augmented Generation (RAG) system for Q&A on docs (e.g., PDF corpus) using LocalStack to emulate AWS services. Use LangChain for chaining, FAISS (local vector DB) for embeddings, and LocalStack Lambda for "serverless" inference. Integrate local Kafka (via Docker) for data ingestion; monitor with Grafana (Dockerized). Handle 1k simulated queries/hr with <200ms latency via local benchmarking. Twist: Rust module for secure vector embedding. Demo: Local API endpoints with curl tests. | Serverless emulation (LocalStack), Local vector DBs, Data streaming (Docker Kafka), Observability.       | 3-5 weeks<br>Python/Rust, LangChain, LocalStack, Docker Kafka, Grafana.                | - Tutorial: LocalStack quickstart (docs.localstack.cloud/user-guide).<br>- Book: "Building Serverless Applications with Python" by Ariel Gonzalez (Ch. on local emulation; free PDF).<br>- Video: "LocalStack + LangChain RAG" (YouTube, search free tutorials, 1 hr). |
| **3**     | **Local Distributed LLM Fine-Tuning Cluster**<br>Set up a Ray cluster on your multi-core machine (or Colab for GPU) for fine-tuning a small LLM (e.g., Phi-2 via Hugging Face). Use local Airflow (Docker) for scheduling, DVC for data versioning, and Prometheus (Docker) for metrics (e.g., throughput). Simulate spot instances with resource throttling. Twist: Go-based fault-tolerant scheduler for node management. Demo: Local training runs with metric dashboards. (Use Colab free GPU for heavy lifts, export to local.)                                 | Distributed computing (Ray local), GPU sim (Colab), Workflow automation (Airflow Docker), Cost mgmt sim. | 5-7 weeks<br>Python/Go, Ray (local/Colab), Docker Airflow, DVC.                        | - Tutorial: Ray local cluster guide (docs.ray.io/en/latest/cluster/getting-started.html).<br>- Book: "Distributed Machine Learning Patterns" by Yuan Tang (Ch. 3-5; free via archive.org).<br>- Video: "Ray on Local Machine" (YouTube, Anyscale channel, 1 hr).       |
| **4**     | **Local AI-Driven Predictive Analytics Dashboard**<br>Create a dashboard for IoT failure prediction (e.g., synthetic sensor data via MQTT broker in Docker). Train XGBoost model locally (scikit-learn), "deploy" via Minikube with Helm charts, and visualize in Streamlit (local server). Include anomaly detection and local alerts (e.g., print/email sim). Twist: Integrate your SQLite project for edge caching in Rust. Demo: Run dashboard on localhost with sample data feeds.                                                                              | Local ML training, Dashboarding (Streamlit), Edge computing sim (Docker MQTT), Helm for K8s.             | 4-6 weeks<br>Python/Rust, scikit-learn/XGBoost, Minikube/Helm, Streamlit, Docker MQTT. | - Tutorial: Streamlit docs (docs.streamlit.io/get-started).<br>- Book: "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow" by Aurélien Géron (Ch. 9-10; free PDF).<br>- Video: "Local ML Dashboard with Streamlit" (YouTube, freeCodeCamp, 45 mins).  |

#### Setup Guide for Zero-Cost Local Environment (30 Mins to Start)
1. **Install Basics**: Docker Desktop (free), Minikube (`brew install minikube` on Mac/Linux), GitHub account (free Actions).
2. **Emulators**: LocalStack (`pip install localstack`), K3s (`curl -sfL https://get.k3s.io | sh -` for lightweight K8s).
3. **GPU Access**: Google Colab (colab.research.google.com; free T4 GPU for 12 hrs/session).
4. **Testing**: Use `htop` for resource monitoring; all tools pip-installable (e.g., `pip install ray[default] langchain`).
5. **Portfolio Polish**: Host code on GitHub; add a `demo.md` with GIFs (via asciinema for terminal recs).

These local versions are interview-gold—recruiters love self-reliant engineers. They'll cover 80% of skills without a dime. Pick #1 to kick off? Or need install scripts?